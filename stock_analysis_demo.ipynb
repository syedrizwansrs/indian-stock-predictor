{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbe8e6a",
   "metadata": {},
   "source": [
    "# Indian Stock Market Analysis & Prediction - Complete Demo\n",
    "\n",
    "This comprehensive notebook demonstrates the complete workflow of the Indian Stock Market Predictor application, from data acquisition to machine learning predictions.\n",
    "\n",
    "## 📊 Overview\n",
    "\n",
    "We'll cover:\n",
    "1. **Data Acquisition**: Fetching Indian stock data from multiple sources\n",
    "2. **Technical Analysis**: Computing 20+ technical indicators\n",
    "3. **Data Visualization**: Creating interactive charts and analysis\n",
    "4. **Feature Engineering**: Preparing data for machine learning\n",
    "5. **Model Training**: Training multiple ML algorithms\n",
    "6. **Model Evaluation**: Comprehensive performance analysis\n",
    "7. **Predictions**: Making next-day price direction predictions\n",
    "\n",
    "⚠️ **Disclaimer**: This is for educational purposes only. Not financial advice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b54549",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data manipulation, analysis, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Technical analysis\n",
    "import pandas_ta as ta\n",
    "\n",
    "# Our custom modules\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from src.data_fetcher import DataFetcher\n",
    "from src.technical_analysis import TechnicalAnalyzer\n",
    "from src.visualization import StockVisualizer\n",
    "from src.prediction_model import StockPredictor\n",
    "from src.config import Config\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📅 Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🐍 Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"📊 Pandas Version: {pd.__version__}\")\n",
    "print(f\"🤖 Sklearn Version: {sklearn.__version__}\")\n",
    "print(f\"📈 XGBoost Version: {xgb.__version__}\")\n",
    "print(f\"🚀 Plotly Version: {plotly.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2e712",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "Let's initialize our modules and fetch data for a popular Indian stock (Reliance Industries) to demonstrate the complete analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our modules\n",
    "print(\"🔧 Initializing modules...\")\n",
    "data_fetcher = DataFetcher()\n",
    "technical_analyzer = TechnicalAnalyzer()\n",
    "visualizer = StockVisualizer()\n",
    "predictor = StockPredictor()\n",
    "\n",
    "# Choose a stock for analysis - Reliance Industries (most liquid Indian stock)\n",
    "STOCK_SYMBOL = \"RELIANCE.BSE\"\n",
    "print(f\"📈 Analyzing: {STOCK_SYMBOL}\")\n",
    "\n",
    "# Fetch or load existing data\n",
    "print(\"📥 Fetching stock data...\")\n",
    "stock_data = data_fetcher.update_stock_data(STOCK_SYMBOL)\n",
    "\n",
    "if stock_data is not None:\n",
    "    print(f\"✅ Successfully loaded {len(stock_data)} records\")\n",
    "    print(f\"📅 Date Range: {stock_data.index[0].date()} to {stock_data.index[-1].date()}\")\n",
    "    print(f\"📊 Data Shape: {stock_data.shape}\")\n",
    "    print(\"\\n📋 Data Sample:\")\n",
    "    display(stock_data.head())\n",
    "    print(\"\\n📊 Data Info:\")\n",
    "    display(stock_data.info())\n",
    "    print(\"\\n📈 Statistical Summary:\")\n",
    "    display(stock_data.describe())\n",
    "else:\n",
    "    print(\"❌ Failed to fetch data. Please check your API key and internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of the raw data\n",
    "if stock_data is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Price chart\n",
    "    axes[0, 0].plot(stock_data.index, stock_data['close'], color='blue', linewidth=1)\n",
    "    axes[0, 0].set_title(f'{STOCK_SYMBOL} - Closing Price')\n",
    "    axes[0, 0].set_ylabel('Price (₹)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volume chart\n",
    "    axes[0, 1].bar(stock_data.index, stock_data['volume'], alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_title(f'{STOCK_SYMBOL} - Trading Volume')\n",
    "    axes[0, 1].set_ylabel('Volume')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Returns histogram\n",
    "    returns = stock_data['close'].pct_change().dropna()\n",
    "    axes[1, 0].hist(returns, bins=50, alpha=0.7, color='green')\n",
    "    axes[1, 0].set_title('Daily Returns Distribution')\n",
    "    axes[1, 0].set_xlabel('Daily Returns')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Price vs Volume scatter\n",
    "    axes[1, 1].scatter(stock_data['volume'], stock_data['close'], alpha=0.5, color='red')\n",
    "    axes[1, 1].set_title('Price vs Volume Relationship')\n",
    "    axes[1, 1].set_xlabel('Volume')\n",
    "    axes[1, 1].set_ylabel('Price (₹)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key statistics\n",
    "    print(\"📊 Key Statistics:\")\n",
    "    print(f\"💰 Current Price: ₹{stock_data['close'].iloc[-1]:.2f}\")\n",
    "    print(f\"📈 52-Week High: ₹{stock_data['close'].max():.2f}\")\n",
    "    print(f\"📉 52-Week Low: ₹{stock_data['close'].min():.2f}\")\n",
    "    print(f\"📊 Average Volume: {stock_data['volume'].mean():,.0f}\")\n",
    "    print(f\"📈 Total Return: {((stock_data['close'].iloc[-1] / stock_data['close'].iloc[0]) - 1) * 100:.2f}%\")\n",
    "    print(f\"📊 Volatility (std): {returns.std() * 100:.2f}%\")\n",
    "    print(f\"📈 Sharpe Ratio: {returns.mean() / returns.std() if returns.std() > 0 else 0:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4e4b3",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Before we can analyze the data, we need to clean it and handle any missing values or anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179dc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality issues\n",
    "if stock_data is not None:\n",
    "    print(\"🔍 Data Quality Check:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = stock_data.isnull().sum()\n",
    "    print(\"📊 Missing Values:\")\n",
    "    print(missing_values)\n",
    "    \n",
    "    # Check for duplicate dates\n",
    "    duplicate_dates = stock_data.index.duplicated().sum()\n",
    "    print(f\"\\n📅 Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Check for outliers (using IQR method)\n",
    "    Q1 = stock_data['close'].quantile(0.25)\n",
    "    Q3 = stock_data['close'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = stock_data[(stock_data['close'] < lower_bound) | (stock_data['close'] > upper_bound)]\n",
    "    print(f\"📊 Price outliers (IQR method): {len(outliers)}\")\n",
    "    \n",
    "    # Check for zero volume days\n",
    "    zero_volume_days = (stock_data['volume'] == 0).sum()\n",
    "    print(f\"📊 Zero volume days: {zero_volume_days}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(\"\\n🔧 Data Preprocessing:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a clean copy\n",
    "    clean_data = stock_data.copy()\n",
    "    \n",
    "    # Handle missing values (forward fill for small gaps)\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"🔧 Filling missing values...\")\n",
    "        clean_data = clean_data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    initial_length = len(clean_data)\n",
    "    clean_data = clean_data.dropna()\n",
    "    dropped_rows = initial_length - len(clean_data)\n",
    "    if dropped_rows > 0:\n",
    "        print(f\"🗑️ Dropped {dropped_rows} rows with NaN values\")\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in numeric_columns:\n",
    "        clean_data[col] = pd.to_numeric(clean_data[col], errors='coerce')\n",
    "    \n",
    "    # Sort by date to ensure proper order\n",
    "    clean_data = clean_data.sort_index()\n",
    "    \n",
    "    # Basic validation\n",
    "    print(\"✅ Preprocessed data shape:\", clean_data.shape)\n",
    "    print(\"✅ Date range:\", clean_data.index[0].date(), \"to\", clean_data.index[-1].date())\n",
    "    print(\"✅ No missing values:\", clean_data.isnull().sum().sum() == 0)\n",
    "    print(\"✅ Proper data types:\", all(clean_data[col].dtype in [np.float64, np.int64] for col in numeric_columns))\n",
    "    \n",
    "    # Store the clean data\n",
    "    stock_data = clean_data\n",
    "    print(\"\\n✅ Data preprocessing completed successfully!\")\n",
    "else:\n",
    "    print(\"❌ No data to preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd40fe",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Now we'll compute technical indicators and engineer features that will be used for machine learning predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive feature engineering\n",
    "if stock_data is not None:\n",
    "    print(\"🔧 Computing Technical Indicators...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use our technical analyzer to compute all indicators\n",
    "    processed_data = technical_analyzer.process_stock_data(stock_data)\n",
    "    \n",
    "    print(f\"✅ Original features: {stock_data.shape[1]}\")\n",
    "    print(f\"✅ Total features after engineering: {processed_data.shape[1]}\")\n",
    "    print(f\"✅ New features created: {processed_data.shape[1] - stock_data.shape[1]}\")\n",
    "    \n",
    "    # Display the technical indicators computed\n",
    "    indicator_columns = [col for col in processed_data.columns if col not in stock_data.columns]\n",
    "    technical_indicators = [col for col in indicator_columns if not col.startswith('Target')]\n",
    "    \n",
    "    print(f\"\\n📈 Technical Indicators ({len(technical_indicators)}):\")\n",
    "    for i, indicator in enumerate(technical_indicators, 1):\n",
    "        print(f\"{i:2d}. {indicator}\")\n",
    "    \n",
    "    # Show sample of engineered features\n",
    "    print(\"\\n📊 Sample of Engineered Features:\")\n",
    "    feature_sample = processed_data[['close'] + technical_indicators[:10]].tail()\n",
    "    display(feature_sample.round(4))\n",
    "    \n",
    "    # Check data quality after feature engineering\n",
    "    print(f\"\\n📊 Data Quality Check:\")\n",
    "    print(f\"🔍 Shape: {processed_data.shape}\")\n",
    "    print(f\"🔍 NaN values: {processed_data.isnull().sum().sum()}\")\n",
    "    print(f\"🔍 Date range: {processed_data.index[0].date()} to {processed_data.index[-1].date()}\")\n",
    "    \n",
    "    # Show correlation of some key indicators with price\n",
    "    if len(technical_indicators) > 0:\n",
    "        price_correlations = processed_data[technical_indicators + ['close']].corr()['close'].sort_values(ascending=False)\n",
    "        print(f\"\\n📊 Top 10 Features Correlated with Price:\")\n",
    "        display(price_correlations.head(11).round(4))  # 11 to include 'close' itself\n",
    "    \n",
    "    print(\"\\n✅ Feature engineering completed successfully!\")\n",
    "else:\n",
    "    print(\"❌ No data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ea0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "if 'processed_data' in locals() and processed_data is not None:\n",
    "    print(\"📊 Creating Interactive Visualizations...\")\n",
    "    \n",
    "    # Use our visualizer to create charts\n",
    "    candlestick_fig = visualizer.create_candlestick_chart(processed_data, STOCK_SYMBOL)\n",
    "    candlestick_fig.show()\n",
    "    \n",
    "    # Technical indicators chart\n",
    "    indicators_fig = visualizer.create_technical_indicators_chart(processed_data, STOCK_SYMBOL)\n",
    "    indicators_fig.show()\n",
    "    \n",
    "    # Returns distribution\n",
    "    returns_fig = visualizer.create_returns_distribution(processed_data, STOCK_SYMBOL)\n",
    "    returns_fig.show()\n",
    "    \n",
    "    # Correlation heatmap of key indicators\n",
    "    key_indicators = ['RSI', 'MACD', 'SMA_50', 'SMA_200', 'EMA_12', 'EMA_26', 'BB_Upper', 'BB_Lower', 'ATR', 'OBV']\n",
    "    available_indicators = [col for col in key_indicators if col in processed_data.columns]\n",
    "    \n",
    "    if len(available_indicators) > 3:\n",
    "        correlation_fig = visualizer.create_correlation_heatmap(processed_data, available_indicators + ['close'])\n",
    "        correlation_fig.show()\n",
    "    \n",
    "    print(\"✅ Interactive visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419967ba",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Training\n",
    "\n",
    "Now we'll prepare the data for machine learning and train multiple models to predict the next day's price direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ed10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "if 'processed_data' in locals() and processed_data is not None:\n",
    "    print(\"🤖 Preparing Data for Machine Learning...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Prepare features and target using our predictor\n",
    "    X_train, X_test, y_train, y_test, feature_names = predictor.prepare_data(processed_data)\n",
    "    \n",
    "    print(f\"📊 Training set shape: {X_train.shape}\")\n",
    "    print(f\"📊 Test set shape: {X_test.shape}\")\n",
    "    print(f\"📊 Number of features: {len(feature_names)}\")\n",
    "    print(f\"📊 Training period: {X_train.index[0].date()} to {X_train.index[-1].date()}\")\n",
    "    print(f\"📊 Test period: {X_test.index[0].date()} to {X_test.index[-1].date()}\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    print(f\"\\n📊 Target Distribution:\")\n",
    "    print(f\"Training - Up: {y_train.sum()}, Down: {len(y_train) - y_train.sum()}\")\n",
    "    print(f\"Test - Up: {y_test.sum()}, Down: {len(y_test) - y_test.sum()}\")\n",
    "    \n",
    "    # Train all models\n",
    "    print(f\"\\n🚀 Training Multiple ML Models...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    trained_models = predictor.train_all_models(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(f\"\\n✅ Successfully trained {len(trained_models)} models!\")\n",
    "    print(\"📋 Models trained:\", list(trained_models.keys()))\n",
    "else:\n",
    "    print(\"❌ No processed data available for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4319e1",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate the performance of our trained models using various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42709fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "if 'trained_models' in locals() and trained_models:\n",
    "    print(\"📊 Model Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get model comparison\n",
    "    performance_df = predictor.get_model_comparison()\n",
    "    \n",
    "    print(\"🏆 Model Performance Ranking:\")\n",
    "    display(performance_df.round(4))\n",
    "    \n",
    "    # Get feature importance for tree-based models\n",
    "    feature_importance = predictor.get_feature_importance_summary(top_n=15)\n",
    "    \n",
    "    if feature_importance:\n",
    "        print(f\"\\n🎯 Feature Importance Analysis:\")\n",
    "        for model_name, importance in feature_importance.items():\n",
    "            print(f\"\\n📊 Top Features for {model_name}:\")\n",
    "            display(importance.round(4))\n",
    "    \n",
    "    # Visualize model performance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Performance comparison\n",
    "    performance_df.plot(kind='bar', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Model Performance Comparison')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Feature importance (if available)\n",
    "    if feature_importance:\n",
    "        best_model_name = list(feature_importance.keys())[0]\n",
    "        importance_data = feature_importance[best_model_name]\n",
    "        importance_data.plot(kind='barh', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(f'Feature Importance - {best_model_name}')\n",
    "        axes[0, 1].set_xlabel('Importance')\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracies = performance_df['accuracy'].sort_values(ascending=True)\n",
    "    axes[1, 0].barh(range(len(accuracies)), accuracies.values)\n",
    "    axes[1, 0].set_yticks(range(len(accuracies)))\n",
    "    axes[1, 0].set_yticklabels(accuracies.index)\n",
    "    axes[1, 0].set_xlabel('Accuracy')\n",
    "    axes[1, 0].set_title('Model Accuracy Comparison')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    f1_scores = performance_df['f1_score'].sort_values(ascending=True)\n",
    "    axes[1, 1].barh(range(len(f1_scores)), f1_scores.values, color='orange')\n",
    "    axes[1, 1].set_yticks(range(len(f1_scores)))\n",
    "    axes[1, 1].set_yticklabels(f1_scores.index)\n",
    "    axes[1, 1].set_xlabel('F1-Score')\n",
    "    axes[1, 1].set_title('Model F1-Score Comparison')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best model analysis\n",
    "    best_model_name = performance_df.index[0]\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "    print(f\"📊 Accuracy: {performance_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "    print(f\"📊 F1-Score: {performance_df.loc[best_model_name, 'f1_score']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained models available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3270c8",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Let's improve our best model by optimizing its hyperparameters using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "if 'best_model_name' in locals() and 'X_train' in locals():\n",
    "    print(f\"🔧 Hyperparameter Tuning for {best_model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a fresh model instance for tuning\n",
    "    if best_model_name == 'random_forest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        base_model = RandomForestClassifier(random_state=Config.RANDOM_STATE)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 15, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif best_model_name == 'xgboost':\n",
    "        base_model = xgb.XGBClassifier(random_state=Config.RANDOM_STATE, eval_metric='logloss')\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    elif best_model_name == 'lightgbm':\n",
    "        base_model = lgb.LGBMClassifier(random_state=Config.RANDOM_STATE, verbosity=-1)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'num_leaves': [31, 50, 100]\n",
    "        }\n",
    "    else:\n",
    "        print(f\"⚠️ Hyperparameter tuning not implemented for {best_model_name}\")\n",
    "        base_model = None\n",
    "        param_grid = None\n",
    "    \n",
    "    if base_model is not None and param_grid is not None:\n",
    "        # Use TimeSeriesSplit for proper time series validation\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        \n",
    "        print(f\"🔍 Testing {np.prod([len(v) for v in param_grid.values()])} parameter combinations...\")\n",
    "        print(\"⏱️ This may take several minutes...\")\\n        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\\n            base_model, param_grid,\\n            cv=tscv, scoring='accuracy',\\n            n_jobs=-1, verbose=1\\n        )\\n        \\n        grid_search.fit(X_train, y_train)\\n        \\n        print(f\\\"\\\\n✅ Hyperparameter tuning completed!\\\")\\n        print(f\\\"🏆 Best parameters: {grid_search.best_params_}\\\")\\n        print(f\\\"📊 Best cross-validation score: {grid_search.best_score_:.4f}\\\")\\n        \\n        # Evaluate the tuned model\\n        tuned_model = grid_search.best_estimator_\\n        tuned_predictions = tuned_model.predict(X_test)\\n        \\n        tuned_accuracy = accuracy_score(y_test, tuned_predictions)\\n        tuned_f1 = f1_score(y_test, tuned_predictions, average='weighted')\\n        \\n        print(f\\\"\\\\n📊 Tuned Model Performance:\\\")\\n        print(f\\\"📊 Test Accuracy: {tuned_accuracy:.4f}\\\")\\n        print(f\\\"📊 Test F1-Score: {tuned_f1:.4f}\\\")\\n        \\n        # Compare with original model\\n        original_accuracy = performance_df.loc[best_model_name, 'accuracy']\\n        original_f1 = performance_df.loc[best_model_name, 'f1_score']\\n        \\n        print(f\\\"\\\\n📈 Improvement:\\\")\\n        print(f\\\"📊 Accuracy: {original_accuracy:.4f} → {tuned_accuracy:.4f} ({tuned_accuracy - original_accuracy:+.4f})\\\")\\n        print(f\\\"📊 F1-Score: {original_f1:.4f} → {tuned_f1:.4f} ({tuned_f1 - original_f1:+.4f})\\\")\\n        \\n        # Store the tuned model\\n        tuned_model_stored = tuned_model\\n        \\nelse:\\n    print(\\\"❌ No model available for hyperparameter tuning\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171bd4d",
   "metadata": {},
   "source": [
    "## 8. Save and Load Model\n",
    "\n",
    "Finally, let's save our best model for future use and demonstrate how to load it back for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and demonstrate predictions\n",
    "if 'tuned_model_stored' in locals():\n",
    "    final_model = tuned_model_stored\n",
    "    model_name = f\"{best_model_name}_tuned\"\n",
    "elif 'trained_models' in locals() and trained_models:\n",
    "    final_model = trained_models[best_model_name]\n",
    "    model_name = best_model_name\n",
    "else:\n",
    "    final_model = None\n",
    "    model_name = None\n",
    "\n",
    "if final_model is not None:\n",
    "    print(f\"💾 Saving Model: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Save the model using our predictor's save method\n",
    "    predictor.save_model(final_model, model_name)\n",
    "    print(f\"✅ Model saved successfully!\")\n",
    "    \n",
    "    # Demonstrate loading the model\n",
    "    print(f\"\\n📥 Loading Model: {model_name}\")\n",
    "    loaded_model = predictor.load_model(model_name)\n",
    "    print(f\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    # Verify the loaded model works\n",
    "    test_prediction = loaded_model.predict(X_test.head(1))\n",
    "    print(f\"🧪 Test prediction: {test_prediction[0]} ({'UP' if test_prediction[0] == 1 else 'DOWN'})\")\n",
    "    \n",
    "    # Make a prediction for the next day\n",
    "    print(f\"\\n🔮 Making Next Day Prediction\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if 'processed_data' in locals():\n",
    "        prediction_result = predictor.predict_next_day(final_model, processed_data, model_name)\n",
    "        \n",
    "        print(f\"📈 Stock: {STOCK_SYMBOL}\")\n",
    "        print(f\"📅 Prediction Date: {prediction_result['timestamp'].date()}\")\n",
    "        print(f\"🎯 Predicted Direction: {prediction_result['direction']}\")\n",
    "        print(f\"🤖 Model Used: {prediction_result['model']}\")\n",
    "        if prediction_result['confidence']:\n",
    "            print(f\"📊 Confidence: {prediction_result['confidence']:.2%}\")\n",
    "        \n",
    "        # Create prediction visualization\n",
    "        print(f\"\\n📊 Creating Prediction Visualization...\")\n",
    "        \n",
    "        # Get recent data for context\n",
    "        recent_data = processed_data.tail(30)\n",
    "        \n",
    "        # Create a simple prediction chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot recent price\n",
    "        ax.plot(recent_data.index, recent_data['close'], 'b-', linewidth=2, label='Close Price')\n",
    "        \n",
    "        # Add prediction point\n",
    "        last_date = recent_data.index[-1]\n",
    "        next_date = last_date + pd.Timedelta(days=1)\n",
    "        last_price = recent_data['close'].iloc[-1]\n",
    "        \n",
    "        # Estimate next price (this is just for visualization)\n",
    "        price_change = recent_data['close'].pct_change().mean()\n",
    "        direction_multiplier = 1 if prediction_result['direction'] == 'UP' else -1\n",
    "        estimated_next_price = last_price * (1 + direction_multiplier * abs(price_change))\n",
    "        \n",
    "        ax.scatter([next_date], [estimated_next_price], \n",
    "                  color='green' if prediction_result['direction'] == 'UP' else 'red',\n",
    "                  s=100, marker='^' if prediction_result['direction'] == 'UP' else 'v',\n",
    "                  label=f\"Predicted: {prediction_result['direction']}\", zorder=5)\n",
    "        \n",
    "        ax.set_title(f\"{STOCK_SYMBOL} - Price Prediction\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(\"Price (₹)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n📊 Model Summary:\")\n",
    "        print(f\"📈 Total predictions made: 1\")\n",
    "        print(f\"🎯 Prediction confidence: {'High' if prediction_result.get('confidence', 0) > 0.6 else 'Medium' if prediction_result.get('confidence', 0) > 0.5 else 'Low'}\")\n",
    "        print(f\"📊 Features used: {len(feature_names)}\")\n",
    "        print(f\"📅 Training data range: {X_train.index[0].date()} to {X_train.index[-1].date()}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No model available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba43e4",
   "metadata": {},
   "source": [
    "## 🎉 Conclusion\n",
    "\n",
    "Congratulations! You have successfully completed a comprehensive stock market analysis and prediction workflow. Here's what we accomplished:\n",
    "\n",
    "### ✅ What We Did\n",
    "1. **Data Acquisition**: Fetched real Indian stock market data\n",
    "2. **Data Preprocessing**: Cleaned and validated the data\n",
    "3. **Feature Engineering**: Created 50+ technical indicators and features\n",
    "4. **Visualization**: Generated interactive charts and analysis\n",
    "5. **Model Training**: Trained multiple ML algorithms\n",
    "6. **Model Evaluation**: Compared performance using various metrics\n",
    "7. **Hyperparameter Tuning**: Optimized the best model\n",
    "8. **Model Persistence**: Saved and loaded models for future use\n",
    "9. **Predictions**: Made next-day price direction predictions\n",
    "\n",
    "### 📊 Key Insights\n",
    "- Technical indicators provide valuable signals for price prediction\n",
    "- Different ML algorithms have varying performance on financial data\n",
    "- Feature engineering significantly impacts model performance\n",
    "- Time series validation is crucial for financial models\n",
    "- Model interpretability through feature importance helps understand predictions\n",
    "\n",
    "### ⚠️ Important Disclaimers\n",
    "- **This is for educational purposes only**\n",
    "- **Not financial advice** - always consult professionals\n",
    "- **Past performance doesn't guarantee future results**\n",
    "- **Market conditions can change rapidly**\n",
    "- **Use proper risk management in real trading**\n",
    "\n",
    "### 🚀 Next Steps\n",
    "1. **Expand to more stocks**: Analyze portfolio of stocks\n",
    "2. **Real-time predictions**: Implement live data feeds\n",
    "3. **Risk management**: Add position sizing and stop-losses\n",
    "4. **Sentiment analysis**: Incorporate news and social media\n",
    "5. **Advanced models**: Try deep learning approaches (LSTM, CNN)\n",
    "6. **Backtesting**: Implement comprehensive strategy testing\n",
    "7. **Production deployment**: Create automated trading systems\n",
    "\n",
    "### 📚 Further Learning\n",
    "- Study more advanced technical indicators\n",
    "- Learn about portfolio optimization\n",
    "- Explore quantitative finance concepts\n",
    "- Practice with different asset classes\n",
    "- Understand market microstructure\n",
    "\n",
    "Thank you for following along with this comprehensive demo! 🙏"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
